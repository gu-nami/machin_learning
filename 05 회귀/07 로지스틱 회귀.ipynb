{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9668e58d",
   "metadata": {},
   "source": [
    "# 07 로지스틱 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237fab3b",
   "metadata": {},
   "source": [
    "### 로지스틱 회귀( Logistic Regression) 개요\n",
    "\n",
    "로지스틱 회귀는 선형 회귀 방식을 분류에 적용한 알고리즘이다. 즉, 로지스틱 회귀는 분류에 사용된다.\n",
    "\n",
    "로지스틱 회귀가 선형 회귀와 다른 점은 선형 함수의 회귀 최적선을 찾은 것이 아니라, 시그모이드(Sigmoid)함수의 최적선을 찾고 이 시그모이드 함수의 반환 값을 확률로 간주해 확률에 따라 분류를 결정한다는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89edf2c6",
   "metadata": {},
   "source": [
    "![시그모이드](https://mblogthumb-phinf.pstatic.net/MjAyMDA3MDdfMTgx/MDAxNTk0MTMwODg2NzAw.Bgt42rm3pV0xTPfuVjN1UbXw9HchDcAdLdvnsrAQvJ0g.ILAv2yJkoMXNiWHKAUe0QswJWyr84GwwlRbXwxCogKUg.PNG.zzoyou_/sigmoid.png?type=w800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dff2fd5",
   "metadata": {},
   "source": [
    "시그모이드 함수는 z값이 아무리 커지거나 작아져도 항상 0과 1사이의 값을 반환한다. x값이 커지면 1에 근사하며 x값이 작아지면 0에 근사한다. x가 0일 때는 0.5이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b2b323",
   "metadata": {},
   "source": [
    "### 로지스틱 회귀 예측\n",
    "\n",
    "로지스틱 회귀는 주로 이진분류(0과1)에 사용된다. (물론 다중 클래스 분류에도 적용 될 수 있음) \n",
    "\n",
    "로지스틱 회귀에서 예측값은 예측 확률을 의미하며, 예측값 즉 예측 확률이 0.5이상이면 1로 0.5이하이면 0으로 예측한다. 로지스틱 회귀의 예측 확률은 시그모이드 함수의 출력값으로 계산된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f6526d",
   "metadata": {},
   "source": [
    "단순 선형 회귀 $y=w_1x + w_0$가 있다고 할 때 로지스틱 회귀는 0과 1을 예측하기에 단순 회귀식에 적용할 수는 없다. 하지만 Odds(성공확률/실패확률)을 통해 선형 회귀식에 확률을 적용한다.\n",
    "\n",
    "$Odds(p)=p/(1-p)$\n",
    "\n",
    "하지만 확률 p의 범위가 0~1사이이고, 선형 회귀의 반환값인 -무한대, +무한대 값에 대응하기 위해 로그 변환을 수행하고 아래와 같이 선형회귀를 적용한다.\n",
    "\n",
    "$Log(Odds(p))=w_1x+w_0$\n",
    "\n",
    "해당 식을 데이터 값 x의확률 p로 정의하면 아래와 같다.\n",
    "\n",
    "$p(x)= 1 / (1+e ^{-(w _1 x+w _0 )})$\n",
    "\n",
    "`로지스틱 회귀는 학습을 통해 시그모이드 함수의 w를 최적화하여 예측하는 것이다.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fda8271",
   "metadata": {},
   "source": [
    "### 로지스틱 회귀 특징\n",
    "\n",
    "+ 로지스틱 회귀는 가볍고, 빠르며, 이진 분류 예측 성능도 뛰어나다. 특히 희소한 데이터 세트 분류에서 성능이 좋아서 텍스트 분류에 자주 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20052fc5",
   "metadata": {},
   "source": [
    "### 사이킷런 로지스틱 회귀\n",
    "\n",
    "+ 사이킷런은 로지스틱 회귀를 LogisticRegression 클래스로 구현한다.\n",
    "\n",
    "+ LogitsticRegression의 주요 하이퍼 파라미터로 penalty, C, solver가 있다. Penalty는 규제 유형을 설정하며 'l2'로 설정 시 L2규제를, 'l1'으로 설정시 L1 규제를 뜻한다. c는 규제 강도를 조절하는 alpha값의 역수이다. 즉 c=1/alpha이다. c값이 작을 수록 규제 강도가 크다.\n",
    "\n",
    "+ solver는 회귀 계수 최적화를 위한 다양한 최적화 방식이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e724e4a8",
   "metadata": {},
   "source": [
    "### LogisticRegression의 solver 유형\n",
    "+ `lbfgs` : 사이킷런 버전 0.22부터 solver의 기본 설정값이다. 메모리 공간을 절약할 수 있고, CPU 코어 수가 많다면 최적화를 병렬로 수행할 수 있다.\n",
    "+ `liblinear` : 사이킷런 버전 0.21까지에서 solver의 기본 설정값이다. 다차원이고 작은 데이터 세트에서 효과적으로 동작하지만 국소 최적화(Local Minimum)에 이슈가 있고, 병렬로 최적화할 수 없다.\n",
    "+ newton-cg : 좀 더 정교한 최적화를 가능하게 하지만, 대용량의 데이터에서 속도가 많이 느려진다.\n",
    "+ sag L Stochastic Average Gradeint로서 경사 하강법 기반의 최적화를 적용한다. 대용량의 데이터에서 빠르게 최적화한다.\n",
    "+ saga : sag와 유사한 최적화 방식이며 L1정규화를 가능하게 한다.\n",
    "\n",
    "다양한 solver 값들이 있지만 이들간의 성능 차이는 미비하며 일반적으로는 lbfgs 또는 liblinear를 선택하는 것이 대부분이다. 기본값인 lbfgs 보다는 lilblinear가 좀 더 빠르게 수행된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c89a45d",
   "metadata": {},
   "source": [
    "사이킷런의 LogisticRegression 클래스를 이용하여 위스콘신 유방암 데이터 세트 긱반에서 로지스틱 회귀로 암 여부를 판단한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27018b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8027f75",
   "metadata": {},
   "source": [
    "데이터에 정규 분포 형태의 표준 스케일링을 적용한 뒤에 trian_test_split()을 이용해 데이터 세트를 분리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7550c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# StandardScaler( )로 평균이 0, 분산 1로 데이터 분포도 변환\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(cancer.data)\n",
    "\n",
    "X_train , X_test, y_train , y_test = train_test_split(data_scaled, cancer.target, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dd54b3",
   "metadata": {},
   "source": [
    "이제 로지스틱 회귀를 이용해 학습 및 예측을 수행하고, 정확도와 ROC-AUC 값을 구해본다. 먼저 solver 값을 'lgfgs'로 설정하고 성능을 확인해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61cce4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.977, roc_auc:0.972\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# 로지스틱 회귀를 이용하여 학습 및 예측 수행.\n",
    "# solver 인자값을 생성자로 입력하지 않으면 solver='lbfgs'\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train, y_train)\n",
    "lr_preds = lr_clf.predict(X_test)\n",
    "\n",
    "# accuracy와 roc_auc 측정\n",
    "print('accuracy: {0:.3f}, roc_auc:{1:.3f}'.format(accuracy_score(y_test, lr_preds),\n",
    "                                                  roc_auc_score(y_test , lr_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadeb91b",
   "metadata": {},
   "source": [
    "이번에는 서로 다른 solver 값으로 LogisticRegression을 학습하고 성능평가를 해본다. 특정 solver는 최적화에 상대적으로 많은 반복 횟수가 필요하다. 따라서 max_iter값을 600으로 설정한다. max_iter는 solver로 지정된 최적화 알고리즘이 최적 수렴할 수 있는 최대 반복 회수이다. max_iter=600이면 최적화 알고리즘이 수렴할 때까지 최대 600번까지 반복하여 회귀 계수를 최적화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "691b6ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solver:lbfgs, accuracy: 0.977, roc_auc:0.972\n",
      "solver:liblinear, accuracy: 0.982, roc_auc:0.979\n",
      "solver:newton-cg, accuracy: 0.977, roc_auc:0.972\n",
      "solver:sag, accuracy: 0.982, roc_auc:0.979\n",
      "solver:saga, accuracy: 0.982, roc_auc:0.979\n"
     ]
    }
   ],
   "source": [
    "solvers = ['lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga']\n",
    "\n",
    "# 여러개의 solver 값별로 LogisticRegression 학습 후 성능 평가\n",
    "for solver in solvers:\n",
    "    lr_clf = LogisticRegression(solver=solver, max_iter=600)\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    lr_preds = lr_clf.predict(X_test)\n",
    "    \n",
    "    # accuracy와 roc_auc 측정\n",
    "    print('solver:{0}, accuracy: {1:.3f}, roc_auc:{2:.3f}'.format(solver,\n",
    "                                                                  accuracy_score(y_test, lr_preds),\n",
    "                                                                  roc_auc_score(y_test , lr_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde3351d",
   "metadata": {},
   "source": [
    "다음은 GridSearchCV를 이용해 위스콘신 데이터 세트에서 solver, penalty, C를 최적화해본다. \n",
    "\n",
    "L1, L2 규제의 경우 slover 설정에 따라 영향을 받는다. Liblinear, saga의 경우 L1, L2 규제가 모두 가능하지만 lbfgs, newton-cg, sag의 경우는 L2 규제만 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16c60db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적 하이퍼 파라미터:{'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}, 최적 평균 정확도:0.979\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params={'solver':['liblinear', 'lbfgs'],\n",
    "        'penalty':['l2', 'l1'],\n",
    "        'C':[0.01, 0.1, 1, 1, 5, 10]}\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "\n",
    "grid_clf = GridSearchCV(lr_clf, param_grid=params, scoring='accuracy', cv=3 )\n",
    "grid_clf.fit(data_scaled, cancer.target)\n",
    "print('최적 하이퍼 파라미터:{0}, 최적 평균 정확도:{1:.3f}'.format(grid_clf.best_params_,\n",
    "                                                  grid_clf.best_score_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
